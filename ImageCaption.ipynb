{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf1c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7631, -1.7631],\n",
      "          [-1.7485, -1.7339, -1.7339,  ..., -1.7923, -1.7923, -1.7923],\n",
      "          [-1.7193, -1.7047, -1.7047,  ..., -1.6317, -1.6171, -1.6609],\n",
      "          ...,\n",
      "          [-1.7777, -1.7777, -1.7631,  ..., -1.7631, -1.7777, -1.7777],\n",
      "          [-1.7631, -1.7631, -1.7631,  ..., -1.7777, -1.7777, -1.7923],\n",
      "          [-1.7923, -1.7777, -1.7777,  ..., -1.7777, -1.7777, -1.7485]],\n",
      "\n",
      "         [[-1.7521, -1.7521, -1.7521,  ..., -1.7371, -1.7371, -1.7071],\n",
      "          [-1.7071, -1.7071, -1.6921,  ..., -1.7521, -1.7521, -1.7521],\n",
      "          [-1.6771, -1.6921, -1.6771,  ..., -1.6170, -1.5570, -1.6170],\n",
      "          ...,\n",
      "          [-1.7521, -1.7521, -1.7521,  ..., -1.7371, -1.7371, -1.7521],\n",
      "          [-1.7371, -1.7521, -1.7221,  ..., -1.7521, -1.7221, -1.7371],\n",
      "          [-1.7521, -1.7521, -1.7371,  ..., -1.7521, -1.7371, -1.7371]],\n",
      "\n",
      "         [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4376, -1.4660],\n",
      "          [-1.4518, -1.4091, -1.4091,  ..., -1.4802, -1.4802, -1.4802],\n",
      "          [-1.4091, -1.3807, -1.3807,  ..., -1.3096, -1.2811, -1.3380],\n",
      "          ...,\n",
      "          [-1.4802, -1.4802, -1.4660,  ..., -1.4660, -1.4518, -1.4802],\n",
      "          [-1.4660, -1.4802, -1.4518,  ..., -1.4802, -1.4660, -1.4660],\n",
      "          [-1.4802, -1.4802, -1.4518,  ..., -1.4660, -1.4518, -1.4802]]],\n",
      "\n",
      "\n",
      "        [[[-1.2959,  0.0471,  1.6092,  ...,  1.9303,  1.5362,  0.3829],\n",
      "          [-1.4419, -0.0405,  1.6530,  ...,  1.9303,  1.4924,  0.2807],\n",
      "          [-1.3981, -0.1134,  1.6238,  ...,  1.9157,  1.5362,  0.3683],\n",
      "          ...,\n",
      "          [-1.7923, -1.7631, -1.7777,  ..., -1.7777, -1.7923, -1.7923],\n",
      "          [-1.7631, -1.7485, -1.7631,  ..., -1.7631, -1.7923, -1.7923],\n",
      "          [-1.7777, -1.7631, -1.7777,  ..., -1.7777, -1.7777, -1.7339]],\n",
      "\n",
      "         [[-1.3169,  0.1839,  1.8348,  ...,  2.0749,  1.7447,  0.5741],\n",
      "          [-1.4369,  0.0939,  1.8798,  ...,  2.0749,  1.6997,  0.6041],\n",
      "          [-1.4669,  0.0038,  1.8648,  ...,  2.0599,  1.7297,  0.6792],\n",
      "          ...,\n",
      "          [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "          [-1.7371, -1.7521, -1.7371,  ..., -1.7521, -1.7521, -1.7371],\n",
      "          [-1.7371, -1.7521, -1.7371,  ..., -1.7521, -1.7521, -1.7521]],\n",
      "\n",
      "         [[-1.0394,  0.4253,  1.9753,  ...,  2.1459,  1.8046,  0.7381],\n",
      "          [-1.0678,  0.3684,  2.0179,  ...,  2.1317,  1.7620,  0.7950],\n",
      "          [-1.0678,  0.2546,  2.0321,  ...,  2.1175,  1.7904,  0.8661],\n",
      "          ...,\n",
      "          [-1.4802, -1.4660, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "          [-1.4518, -1.4518, -1.4518,  ..., -1.4802, -1.4802, -1.4660],\n",
      "          [-1.4518, -1.4802, -1.4660,  ..., -1.4802, -1.4802, -1.4802]]],\n",
      "\n",
      "\n",
      "        [[[-0.5660, -0.5806, -0.5806,  ..., -0.6098, -0.6098, -0.6098],\n",
      "          [-0.5806, -0.5514, -0.5514,  ..., -0.5952, -0.6098, -0.6244],\n",
      "          [-0.5660, -0.5514, -0.5514,  ..., -0.5806, -0.5806, -0.6098],\n",
      "          ...,\n",
      "          [-0.5514, -0.5222, -0.5368,  ..., -0.5514, -0.5660, -0.5660],\n",
      "          [-0.5222, -0.5222, -0.5222,  ..., -0.5368, -0.5514, -0.5514],\n",
      "          [-0.5660, -0.5514, -0.5368,  ..., -0.5514, -0.5660, -0.5368]],\n",
      "\n",
      "         [[-0.4614, -0.5065, -0.4914,  ..., -0.5365, -0.5365, -0.5215],\n",
      "          [-0.5065, -0.4914, -0.5065,  ..., -0.5365, -0.5515, -0.5515],\n",
      "          [-0.5065, -0.4914, -0.4914,  ..., -0.5365, -0.5365, -0.5365],\n",
      "          ...,\n",
      "          [-0.4764, -0.4764, -0.4764,  ..., -0.4914, -0.4914, -0.4914],\n",
      "          [-0.4614, -0.4614, -0.4764,  ..., -0.4914, -0.4914, -0.4614],\n",
      "          [-0.4614, -0.5065, -0.4914,  ..., -0.4914, -0.4914, -0.4614]],\n",
      "\n",
      "         [[-0.2857, -0.3000, -0.2857,  ..., -0.3284, -0.3142, -0.3142],\n",
      "          [-0.2715, -0.2715, -0.2715,  ..., -0.3142, -0.3142, -0.3284],\n",
      "          [-0.2857, -0.2715, -0.2857,  ..., -0.3142, -0.3000, -0.3142],\n",
      "          ...,\n",
      "          [-0.2573, -0.2573, -0.2573,  ..., -0.2857, -0.2857, -0.2857],\n",
      "          [-0.2431, -0.2431, -0.2715,  ..., -0.2715, -0.2857, -0.2715],\n",
      "          [-0.2431, -0.2715, -0.2573,  ..., -0.2715, -0.2715, -0.2715]]],\n",
      "\n",
      "\n",
      "        [[[-1.7631, -1.7485, -1.7485,  ..., -1.7047, -1.6609, -1.6463],\n",
      "          [-1.7047, -1.5149, -1.3105,  ..., -1.4857, -1.5295, -1.5733],\n",
      "          [-1.7047, -1.4711, -1.2521,  ..., -1.4711, -1.4711, -1.5441],\n",
      "          ...,\n",
      "          [-1.5733, -1.0331, -0.2594,  ...,  0.0033, -0.3762, -1.2229],\n",
      "          [-1.4857, -1.0623, -0.2740,  ...,  0.0325, -0.2886, -1.1645],\n",
      "          [-1.4857, -1.1645, -0.3616,  ...,  0.0617, -0.2886, -1.0331]],\n",
      "\n",
      "         [[-1.7221, -1.7071, -1.7071,  ..., -1.6771, -1.6170, -1.6020],\n",
      "          [-1.6771, -1.4669, -1.2418,  ..., -1.4669, -1.4669, -1.5570],\n",
      "          [-1.6621, -1.4219, -1.1968,  ..., -1.4219, -1.4069, -1.4820],\n",
      "          ...,\n",
      "          [-1.5720, -0.9867, -0.1763,  ...,  0.0638, -0.2663, -1.1218],\n",
      "          [-1.5120, -1.0317, -0.2063,  ...,  0.0939, -0.2063, -1.0467],\n",
      "          [-1.4669, -1.1668, -0.2513,  ...,  0.1689, -0.1763, -0.9417]],\n",
      "\n",
      "         [[-1.4660, -1.4376, -1.4233,  ..., -1.4233, -1.3522, -1.4091],\n",
      "          [-1.4233, -1.1816, -0.9683,  ..., -1.1816, -1.1958, -1.2811],\n",
      "          [-1.4233, -1.1674, -0.9256,  ..., -1.1389, -1.1247, -1.2100],\n",
      "          ...,\n",
      "          [-1.3665, -0.7550,  0.0413,  ...,  0.2831, -0.0724, -0.9114],\n",
      "          [-1.2811, -0.8119,  0.0271,  ...,  0.2973,  0.0413, -0.8119],\n",
      "          [-1.2243, -0.9825, -0.0724,  ...,  0.3399,  0.0555, -0.7692]]]]), 'input_ids': tensor([[  101,  2023,  5810, 21419,  2557, 14413,  1997,  2019,  2055,  1015,\n",
      "          1011,  3204,  1011,  2214,  3287, 10527,  3065, 29295,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  5810, 21419,  2557, 14413,  7657, 29295,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1037,  5810, 21419,  2557, 14413,  1997,  2019,  2055,  1015,\n",
      "          1011,  3204,  1011,  2214,  3287, 10527,  2007, 17844,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  5810, 21419,  2557, 14413,  7657,  8777,  4308, 29454,\n",
      "          3370,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[  101,  2023,  5810, 21419,  2557, 14413,  1997,  2019,  2055,  1015,\n",
      "          1011,  3204,  1011,  2214,  3287, 10527,  3065, 29295,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [  101,  2023,  5810, 21419,  2557, 14413,  7657, 29295,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [  101,  1037,  5810, 21419,  2557, 14413,  1997,  2019,  2055,  1015,\n",
      "          1011,  3204,  1011,  2214,  3287, 10527,  2007, 17844,  4487, 16173,\n",
      "          5732,  4308,  1012,   102,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [  101,  2023,  5810, 21419,  2557, 14413,  7657,  8777,  4308, 29454,\n",
      "          3370,  1012,   102,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hyssk\\AppData\\Local\\Temp\\ipykernel_14580\\2867828739.py:72: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class ImageCaptioningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, processor, image_root):\n",
    "        self.dataset = joblib.load(dataset_path)\n",
    "        self.processor = processor\n",
    "        self.image_root = image_root\n",
    "        self.W, self.H = 512, 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx]\n",
    "        caption = row['Caption']\n",
    "        image_path = os.path.join(self.image_root, row['ImagePath'], row['Filename'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=32,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = inputs.input_ids.clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": inputs.pixel_values.squeeze(),\n",
    "            \"input_ids\": inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = ImageCaptioningDataset('Train_Data.pkl',processor,'../Data/Training/01.원천데이터')\n",
    "valid_dataset = ImageCaptioningDataset('Validation_Data.pkl',processor,'../Data/Validation/01.원천데이터/')\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 3\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2b03c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...:   0%|          | 0/2000 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training batch: ...: 100%|██████████| 2000/2000 [09:45<00:00,  3.41it/s]\n",
      "Validating batch: ...: 100%|██████████| 250/250 [02:48<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Training loss: 0.353774195343256 - Eval Loss: 2.215343067228794 - LR: 4e-05\n",
      "Saved model to blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|██████████| 2000/2000 [17:14<00:00,  1.93it/s]\n",
      "Validating batch: ...: 100%|██████████| 250/250 [02:42<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 - Training loss: 0.2784610586836934 - Eval Loss: 1.6091344608068465 - LR: 3.6e-05\n",
      "Saved model to blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|██████████| 2000/2000 [17:22<00:00,  1.92it/s]\n",
      "Validating batch: ...: 100%|██████████| 250/250 [02:41<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 - Training loss: 0.2630824041739106 - Eval Loss: 1.1490215846896172 - LR: 3.24e-05\n",
      "Saved model to blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|██████████| 2000/2000 [17:04<00:00,  1.95it/s]\n",
      "Validating batch: ...: 100%|██████████| 250/250 [02:40<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 - Training loss: 0.25818563921749593 - Eval Loss: 1.146082087814808 - LR: 2.9160000000000002e-05\n",
      "Saved model to blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...: 100%|██████████| 2000/2000 [16:31<00:00,  2.02it/s]\n",
      "Validating batch: ...: 100%|██████████| 250/250 [02:45<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 - Training loss: 0.25346651002764703 - Eval Loss: 1.044375267982483 - LR: 2.6244e-05\n",
      "Saved model to blip-saved-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch: ...:   2%|▏         | 45/2000 [00:27<19:38,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader)), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining batch: ...\u001b[39m\u001b[38;5;124m'\u001b[39m), train_dataloader):\n\u001b[0;32m      5\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mImageCaptioningDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m caption \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     34\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_root, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImagePath\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 35\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m     38\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage,\n\u001b[0;32m     39\u001b[0m     text\u001b[38;5;241m=\u001b[39mcaption,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\PIL\\Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyssk\\anaconda3\\envs\\MedicalProject\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        # attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "            \n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for idx, batch in zip(tqdm(range(len(valid_dataloader)), desc='Validating batch: ...'), valid_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    scheduler.step()\n",
    "    if eval_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"blip-saved-model\", from_pt=True) \n",
    "        print(\"Saved model to blip-saved-model\")\n",
    "        min_eval_loss = eval_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "    \n",
    "# pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "# print(\"The finetuning process has done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d696954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 캡션: this plain abdominal radiograph shows a non - specific bowel gas pattern.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# 디바이스 설정 (CUDA or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 저장한 모델 및 Processor 불러오기\n",
    "model = BlipForConditionalGeneration.from_pretrained(r\"C:\\Users\\hyssk\\MedicalProjects\\blip-saved-model\").to(device)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# 추론할 이미지 로드\n",
    "image_path = r\"C:\\Users\\hyssk\\MedicalProjects\\Data\\Validation\\01.원천데이터\\VS_2.정상\\5_1760.png\"  # 테스트할 이미지 경로\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Processor를 사용하여 입력값 생성\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델 추론\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"생성된 캡션:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedicalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
